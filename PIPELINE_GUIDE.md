# Complete Pipeline Guide: Facial Emotion Detection System

> **A step-by-step walkthrough from environment setup to production deployment**

This guide provides a complete, reproducible pipeline for building and deploying the facial emotion detection system. Follow these steps in order to replicate the project from scratch.

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Environment Setup](#environment-setup)
3. [Phase 1: Data Preparation](#phase-1-data-preparation)
4. [Phase 2: Model Validation](#phase-2-model-validation)
5. [Phase 3: Training Pipeline](#phase-3-training-pipeline)
6. [Phase 4: Evaluation & Testing](#phase-4-evaluation--testing)
7. [Phase 5: Deployment](#phase-5-deployment)
8. [Troubleshooting](#troubleshooting)
9. [Advanced Usage](#advanced-usage)

---

## Quick Start

**TL;DR** - Run this if you already have the dataset and environment ready:

```bash
# 1. Preprocess dataset
python preprocess_dataset.py --input_dir dataset --output_dir preprocessed_data --detector mtcnn

# 2. Create splits
python split_dataset.py --input_dir preprocessed_data --output_dir data_splits

# 3. Train models (3-stage pipeline, ~60 mins total)
python train_phase3a.py --epochs 20 --models resnet50
python train_phase3b_progressive.py --epochs 20 --models resnet50
python train_phase3c_deep.py --epochs 20 --models resnet50

# 4. Evaluate
python evaluate_models.py --models resnet50

# 5. Test with webcam
python webcam_demo.py --model resnet50

# 6. Export for deployment
python export_models.py --models resnet50 --validate --benchmark
```

---

## Environment Setup

### 1. System Requirements

**Minimum Requirements**:
- **OS**: Windows 10/11, Linux (Ubuntu 20.04+), or macOS
- **Python**: 3.8 or higher
- **RAM**: 8 GB minimum (16 GB recommended)
- **Storage**: 5 GB free space

**Recommended for Training**:
- **GPU**: NVIDIA GPU with 4+ GB VRAM (RTX 3060 or better)
- **CUDA**: 11.8 or higher
- **cuDNN**: Compatible version with CUDA

**Check GPU availability**:
```bash
# Windows
nvidia-smi

# Python
python -c "import torch; print(torch.cuda.is_available())"
```

### 2. Create Virtual Environment

**Option A: Using venv (Recommended)**
```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\Activate.ps1

# Activate (Linux/Mac)
source .venv/bin/activate
```

**Option B: Using conda**
```bash
conda create -n emotion-detection python=3.10
conda activate emotion-detection
```

### 3. Install Dependencies

**Core Dependencies** (Required for all phases):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install opencv-python pillow numpy tqdm matplotlib seaborn
pip install scikit-learn pandas
```

**Optional Dependencies** (For better face detection):
```bash
pip install facenet-pytorch
```

**Phase 5 Dependencies** (For deployment):
```bash
pip install onnx onnxruntime
```

**Verify Installation**:
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

Expected output:
```
PyTorch: 2.1.0+cu118
CUDA: True
```

### 4. Project Structure Setup

```bash
# Clone or download the project
git clone <repository-url>
cd edl-project_facial-emotion-recognition

# Verify project structure
ls
# Should see: models/, config/, utils/, scripts/, etc.
```

---

## Phase 1: Data Preparation

### Step 1: Organize Raw Dataset

**Assumed starting structure**:
```
dataset/
â”œâ”€â”€ d1/
â”‚   â”œâ”€â”€ 23XXX-01-AN-01.jpg
â”‚   â”œâ”€â”€ 23XXX-01-HA-02.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ d2/
â””â”€â”€ ...
```

**1.1 Clean macOS Metadata** (if applicable):
```powershell
.\scripts\clean_mac_files.ps1
```

**1.2 Validate and Fix Naming**:
```powershell
# Dry run to preview changes
.\scripts\validate_naming.ps1 -DryRun

# Apply fixes
.\scripts\validate_naming.ps1
```

**1.3 Organize into Emotion Folders**:
```powershell
.\scripts\organize_dataset.ps1
```

**Expected result**:
```
dataset/
â”œâ”€â”€ d1/
â”‚   â”œâ”€â”€ angry/
â”‚   â”‚   â”œâ”€â”€ 23XXX-01-AN-01.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ happy/
â”‚   â”œâ”€â”€ neutral/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

### Step 2: Preprocess Images

**Preprocess with MTCNN** (Best accuracy):
```bash
python preprocess_dataset.py \
    --input_dir dataset \
    --output_dir preprocessed_data \
    --detector mtcnn \
    --margin 0.2 \
    --size 224
```

**Alternative: Preprocess with Haar Cascade** (No extra dependencies):
```bash
python preprocess_dataset.py \
    --input_dir dataset \
    --output_dir preprocessed_data \
    --detector haar
```

**What it does**:
- Detects faces in each image
- Crops face with 20% margin
- Resizes to 224Ã—224 pixels
- Saves to `preprocessed_data/` with same structure
- Generates `preprocessing_stats.json` with statistics

**Expected output**:
```
Processing d1/angry: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450/450 [00:45<00:00, 10.00it/s]
Processing d1/happy: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:52<00:00, 10.00it/s]
...
Successfully preprocessed: 3198/3241 images (98.7%)
Statistics saved to: preprocessed_data/preprocessing_stats.json
```

**Verify preprocessing**:
```bash
# Check statistics
cat preprocessed_data/preprocessing_stats.json

# View a sample image
python -c "from PIL import Image; Image.open('preprocessed_data/d1/happy/23XXX-01-HA-01.jpg').show()"
```

### Step 3: Create Train/Val/Test Splits

**Create balanced splits** (80% train, 10% val, 10% test):
```bash
python split_dataset.py \
    --input_dir preprocessed_data \
    --output_dir data_splits \
    --train_ratio 0.8 \
    --val_ratio 0.1 \
    --seed 42
```

**Expected output**:
```
Total images: 3198
Train: 2559 (80.0%)
Val: 320 (10.0%)
Test: 319 (10.0%)

Per-emotion distribution:
  angry:     train=365, val=46, test=46
  disgust:   train=338, val=42, test=42
  fear:      train=343, val=43, test=43
  happy:     train=470, val=59, test=59
  neutral:   train=249, val=31, test=31
  sad:       train=377, val=47, test=47
  surprised: train=417, val=52, test=51

Split info saved to: data_splits/split_info.json
```

**Verify splits**:
```bash
# Check split info
cat data_splits/split_info.json

# Count images
ls data_splits/train/*/*.jpg | wc -l  # Should be ~2559
ls data_splits/val/*/*.jpg | wc -l    # Should be ~320
ls data_splits/test/*/*.jpg | wc -l   # Should be ~319
```

### Step 4: Validate Training Setup

**Optional: Run pre-training validation**:
```bash
python validate_training_setup.py
```

**What it checks**:
- Dataset structure and accessibility
- Image format consistency
- Class balance
- GPU availability
- Model instantiation

---

## Phase 2: Model Validation

### Verify All Models Work

**Test all 4 models**:
```bash
python test_models.py
```

**Expected output**:
```
======================================================================
TESTING: MOBILENET
======================================================================
âœ“ Model created successfully
âœ“ Forward pass successful
âœ“ Output shape correct: torch.Size([4, 7])
âœ“ Freeze/unfreeze working
âœ“ Device: cuda:0

======================================================================
MODEL COMPARISON
======================================================================
Model                Total Params    Status     Notes
----------------------------------------------------------------------
mobilenet            2,388,743       âœ“ PASS
resnet18             11,243,079      âœ“ PASS
resnet50             23,771,207      âœ“ PASS
efficientnet         4,172,419       âœ“ PASS
======================================================================

Summary: 4/4 models passed all tests
âœ“ All models are ready for Phase 3 training!
```

---

## Phase 3: Training Pipeline

### Overview: 3-Stage Progressive Training

**Stage 1 (Warmup)**: Frozen backbone, train classifier only (~20 mins)  
**Stage 2 (Progressive)**: Partial backbone unfreezing (~20 mins)  
**Stage 3 (Deep)**: Full network fine-tuning (~20 mins)  

**Total training time**: ~60 minutes for one model on RTX 4060

---

### Stage 1: Warmup Training

**Train ResNet50** (recommended for best accuracy):
```bash
python train_phase3a.py --epochs 20 --models resnet50
```

**Train all models**:
```bash
python train_phase3a.py --epochs 20 --models mobilenet,efficientnet,resnet18,resnet50
```

**What happens**:
- Backbone frozen (ImageNet weights preserved)
- Only custom classifier trained
- Conservative augmentation (rotation Â±10Â°)
- Learning rate: 1Ã—10â»â´
- Early stopping patience: 20 epochs

**Expected output**:
```
=== Warm-up Training: resnet50 ===
[INFO] Frozen backbone layers (including BatchNorm eval mode)
[INFO] Using LinearLR warmup: 1.00e-08 â†’ 1.00e-06 over 3 epochs

Epoch 1/20: Train Loss: 1.8234, Train Acc: 0.2567 | Val Loss: 1.7845, Val Acc: 0.2719
Epoch 5/20: Train Loss: 1.2456, Train Acc: 0.5234 | Val Loss: 1.3567, Val Acc: 0.4875
Epoch 10/20: Train Loss: 0.9234, Train Acc: 0.6789 | Val Loss: 1.1234, Val Acc: 0.5625
...
Best validation accuracy: 0.5891 at epoch 15
Checkpoint saved: results/checkpoints/resnet50_warmup_best.pt
```

**Verify checkpoint**:
```bash
ls results/checkpoints/resnet50_warmup_best.pt
# File should exist, ~90 MB
```

---

### Stage 2: Progressive Fine-Tuning

**Train ResNet50**:
```bash
python train_phase3b_progressive.py --epochs 20 --models resnet50
```

**What happens**:
- Loads Stage 1 checkpoint
- Partially unfreezes backbone:
  - ResNet50: Layer 4 + last 2 blocks of Layer 3
  - ResNet18: Layer 3 + Layer 4
  - MobileNet: Last 4 inverted residual blocks
  - EfficientNet: Last 5 MBConv blocks
- Moderate-aggressive augmentation
- Learning rate: 3Ã—10â»âµ with cosine annealing
- Early stopping patience: 10 epochs

**Expected output**:
```
======================================================================
PROGRESSIVE FINE-TUNING (STAGE 2): RESNET50
======================================================================
[CHECKPOINT] Loading from warmup: results\checkpoints\resnet50_warmup_best.pt
[BASELINE] Warmup validation accuracy: 0.5891
[INFO] Unfroze ResNet50 layer4 + last 2 blocks of layer3
[INFO] Parameters: 52/163 trainable, 111 frozen

Epoch 1/20: Train Loss: 0.8456, Train Acc: 0.6923 | Val Loss: 0.9234, Val Acc: 0.6406
Epoch 5/20: Train Loss: 0.5234, Train Acc: 0.8156 | Val Loss: 0.7123, Val Acc: 0.7531
Epoch 10/20: Train Loss: 0.3456, Train Acc: 0.8789 | Val Loss: 0.6234, Val Acc: 0.7875
...
Best validation accuracy: 0.7906 at epoch 12
Checkpoint saved: results/checkpoints/resnet50_finetune_progressive_best.pt
```

**Verify checkpoint**:
```bash
ls results/checkpoints/resnet50_finetune_progressive_best.pt
```

---

### Stage 3: Deep Fine-Tuning

**Train ResNet50**:
```bash
python train_phase3c_deep.py --epochs 20 --models resnet50 --plot_curves
```

**What happens**:
- Loads Stage 2 checkpoint
- Unfreezes entire backbone (100% trainable)
- Same augmentation as Stage 2
- Learning rate: 3Ã—10â»âµ with cosine annealing
- Early stopping patience: 12 epochs
- Generates training curves if `--plot_curves` specified

**Expected output**:
```
======================================================================
DEEP FINE-TUNING (STAGE 3): RESNET50
======================================================================
[CHECKPOINT] Loading from Stage 2: results\checkpoints\resnet50_finetune_progressive_best.pt
[BASELINE] Stage 2 validation accuracy: 0.7906
[INFO] Parameters: 163/163 trainable (100% unfrozen)

Epoch 1/20: Train Loss: 0.4567, Train Acc: 0.8423 | Val Loss: 0.5678, Val Acc: 0.7969
Epoch 5/20: Train Loss: 0.2345, Train Acc: 0.9123 | Val Loss: 0.4234, Val Acc: 0.8406
Epoch 10/20: Train Loss: 0.1456, Train Acc: 0.9534 | Val Loss: 0.3892, Val Acc: 0.8594
...
Best validation accuracy: 0.8625 at epoch 14
Test accuracy: 0.7440

Checkpoint saved: results/checkpoints/resnet50_finetune_deep_best.pt
Training curves saved: results/curves/resnet50_finetune_deep_curves.png
```

**Final checkpoints created**:
- `results/checkpoints/resnet50_warmup_best.pt` (Stage 1)
- `results/checkpoints/resnet50_finetune_progressive_best.pt` (Stage 2)
- `results/checkpoints/resnet50_finetune_deep_best.pt` (Stage 3 - **FINAL MODEL**)

---

### Training All 4 Models (Optional)

**Full pipeline for all models** (~4 hours total):
```bash
# Stage 1
python train_phase3a.py --epochs 20 --models mobilenet,efficientnet,resnet18,resnet50

# Stage 2
python train_phase3b_progressive.py --epochs 20 --models mobilenet,efficientnet,resnet18,resnet50

# Stage 3
python train_phase3c_deep.py --epochs 20 --models mobilenet,efficientnet,resnet18,resnet50 --plot_curves
```

---

## Phase 4: Evaluation & Testing

### Comprehensive Model Evaluation

**Evaluate all trained models**:
```bash
python evaluate_models.py --models all
```

**Evaluate specific model with predictions export**:
```bash
python evaluate_models.py --models resnet50 --save_predictions
```

**Expected output**:
```
======================================================================
PHASE 4: MODEL EVALUATION AND VISUALIZATION
======================================================================

Evaluating: RESNET50
======================================================================
Loading resnet50 from results\checkpoints\resnet50_finetune_deep_best.pt

Overall Metrics:
  Test Accuracy:  74.40%
  Precision:      74.52%
  Recall:         74.40%
  F1-Score:       74.28%

Per-Emotion Accuracy:
  Angry       : 80.00%
  Disgust     : 70.83%
  Fear        : 57.45%
  Happy       : 80.85%
  Neutral     : 70.83%
  Sad         : 78.72%
  Surprised   : 82.61%

Confusion matrix saved to: results\evaluation\confusion_matrices\resnet50_confusion_matrix.png
```

**View results**:
```bash
# Open confusion matrix
start results/evaluation/confusion_matrices/resnet50_confusion_matrix.png

# View evaluation metrics
cat results/evaluation/evaluation_metrics.json
```

---

### Testing Tools

**1. Single Image Prediction**:
```bash
# Basic prediction
python predict.py --image test_images/happy_face.jpg --model resnet50

# With face detection and visualization
python predict.py --image photo.jpg --model resnet50 --detect_face --visualize --save_viz result.png
```

**2. Batch Folder Processing (Ensemble)**:
```bash
# Process entire folder with all models
python predict_ensemble.py --folder test_images/

# Single image ensemble
python predict_ensemble.py --image test.jpg
```

**3. Real-Time Webcam Demo**:
```bash
# Start webcam with ResNet50
python webcam_demo.py --model resnet50

# Custom camera and confidence threshold
python webcam_demo.py --model resnet18 --camera 0 --threshold 0.5
```

**Webcam Controls**:
- Press `q` to quit
- Press `s` to save screenshot
- Press `f` to toggle face detection boxes

---

## Phase 5: Deployment

### Export Models for Production

**Export all models to ONNX and TorchScript**:
```bash
python export_models.py --models all --validate --benchmark
```

**Export specific model to ONNX only**:
```bash
python export_models.py --models resnet50 --format onnx --validate
```

**Expected output**:
```
======================================================================
EXPORTING: RESNET50
======================================================================
Loading resnet50 from results\checkpoints\resnet50_finetune_deep_best.pt
Model loaded - Val Acc: 0.8625

[ONNX] Exporting resnet50...
[ONNX] Saved to: exported_models\onnx\resnet50.onnx
[ONNX] File size: 90.45 MB

[VALIDATION] Checking ONNX model accuracy...
[VALIDATION] Max difference: 0.000003
[VALIDATION] âœ“ ONNX export validated successfully!

[TorchScript] Exporting resnet50...
[TorchScript] Saved to: exported_models\torchscript\resnet50_traced.pt
[TorchScript] File size: 94.12 MB

[BENCHMARK] Results (average over 100 iterations):
  PyTorch:     8.234 ms
  TorchScript: 7.123 ms (1.16x faster)
  ONNX:        6.456 ms (1.28x faster)

[METADATA] Saved to: exported_models\metadata\resnet50_metadata.json
```

**Exported files**:
```
exported_models/
â”œâ”€â”€ onnx/
â”‚   â””â”€â”€ resnet50.onnx               # For mobile, web, cross-platform
â”œâ”€â”€ torchscript/
â”‚   â””â”€â”€ resnet50_traced.pt          # For C++ production servers
â””â”€â”€ metadata/
    â””â”€â”€ resnet50_metadata.json      # Preprocessing info, class labels
```

---

### Video Emotion Analysis

**Process video file**:
```bash
# Basic video processing
python predict_video.py --video demo.mp4 --model resnet50

# With timeline visualization and frame skipping
python predict_video.py --video long_video.mp4 --skip_frames 2 --save_timeline

# Skip output video creation (faster)
python predict_video.py --video webcam.avi --no_output_video --save_timeline
```

**Expected output**:
```
======================================================================
VIDEO EMOTION DETECTION
======================================================================
Input video: demo.mp4
Model: resnet50
Skip frames: 0 (process every 1 frames)
Smoothing window: 5 frames
======================================================================

Video info: 1920x1080 @ 30 FPS, 900 frames (30.0s)
Output video: video_results\demo\demo_annotated.mp4

Processing video: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [02:15<00:00, 6.67it/s]

[CSV] Saved to: video_results\demo\demo_emotions.csv
[TIMELINE] Saved to: video_results\demo\demo_timeline.png

======================================================================
SUMMARY STATISTICS
======================================================================
Total frames processed: 900
Total face detections: 1847

Emotion Distribution:
  Angry       :   87 ( 4.7%)
  Disgust     :   45 ( 2.4%)
  Fear        :   12 ( 0.6%)
  Happy       :  834 (45.2%)
  Neutral     :  567 (30.7%)
  Sad         :  123 ( 6.7%)
  Surprised   :  179 ( 9.7%)

[SUMMARY] Saved to: video_results\demo\demo_summary.json
======================================================================
```

**Output files**:
```
video_results/demo/
â”œâ”€â”€ demo_annotated.mp4          # Video with emotion labels
â”œâ”€â”€ demo_emotions.csv           # Frame-by-frame data
â”œâ”€â”€ demo_timeline.png           # Emotion distribution plot
â””â”€â”€ demo_summary.json           # Processing statistics
```

---

### Production Deployment Examples

**1. Python ONNX Inference**:
```python
import onnxruntime as ort
import numpy as np
from PIL import Image
import json

# Load model and metadata
session = ort.InferenceSession("exported_models/onnx/resnet50.onnx")
with open("exported_models/metadata/resnet50_metadata.json") as f:
    metadata = json.load(f)

# Preprocess image
def preprocess(image_path):
    img = Image.open(image_path).resize((224, 224))
    img_array = np.array(img).transpose(2, 0, 1).astype(np.float32) / 255.0
    
    # Normalize
    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)
    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)
    img_array = (img_array - mean) / std
    
    return img_array[np.newaxis, :]  # Add batch dimension

# Run inference
input_data = preprocess("test.jpg")
outputs = session.run(None, {"input": input_data})

# Get prediction
emotion_idx = np.argmax(outputs[0])
emotion = metadata['class_labels'][emotion_idx]
confidence = np.exp(outputs[0][0]) / np.sum(np.exp(outputs[0][0]))

print(f"Emotion: {emotion} ({confidence[emotion_idx]*100:.2f}%)")
```

**2. C++ TorchScript Inference**:
```cpp
#include <torch/script.h>
#include <iostream>
#include <memory>

int main() {
    // Load model
    torch::jit::script::Module module;
    try {
        module = torch::jit::load("exported_models/torchscript/resnet50_traced.pt");
    }
    catch (const c10::Error& e) {
        std::cerr << "Error loading model\n";
        return -1;
    }

    // Create input tensor (1, 3, 224, 224)
    auto input = torch::randn({1, 3, 224, 224});
    
    // Run inference
    std::vector<torch::jit::IValue> inputs;
    inputs.push_back(input);
    auto output = module.forward(inputs).toTensor();
    
    // Get prediction
    auto emotion_idx = output.argmax(1).item<int>();
    std::cout << "Predicted emotion index: " << emotion_idx << std::endl;
    
    return 0;
}
```

---

## Troubleshooting

### Common Issues and Solutions

#### Issue 1: CUDA Out of Memory

**Error**:
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GB
```

**Solution**:
```bash
# Reduce batch size in config/model_config.py
BATCH_SIZE = 16  # Default is 32

# Or train on CPU (slower)
USE_GPU = False
```

#### Issue 2: No faces detected during preprocessing

**Error**:
```
Warning: No face detected in image: dataset/d1/angry/23XXX-01-AN-01.jpg
```

**Solution**:
```bash
# Try MTCNN instead of Haar Cascade
python preprocess_dataset.py --detector mtcnn

# Reduce margin if face is too small
python preprocess_dataset.py --margin 0.1

# Lower minimum face size for Haar
python preprocess_dataset.py --detector haar --min_size 30
```

#### Issue 3: Model not improving during training

**Symptoms**:
- Validation accuracy stuck at ~25-30%
- Loss not decreasing

**Solution**:
```bash
# Check if data is loaded correctly
python validate_training_setup.py

# Verify preprocessing was successful
cat preprocessed_data/preprocessing_stats.json

# Ensure splits are balanced
cat data_splits/split_info.json

# Try lower learning rate
# In config/model_config.py:
LEARNING_RATE = 5e-5  # Instead of 1e-4
```

#### Issue 4: Import errors

**Error**:
```
ModuleNotFoundError: No module named 'torch'
```

**Solution**:
```bash
# Activate virtual environment first
.venv\Scripts\Activate.ps1  # Windows
source .venv/bin/activate    # Linux/Mac

# Reinstall dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Issue 5: Webcam not working

**Error**:
```
Error: Cannot open camera
```

**Solution**:
```bash
# Try different camera index
python webcam_demo.py --camera 1

# Check camera permissions (Windows)
# Settings > Privacy > Camera > Allow apps to access camera

# Test camera with OpenCV
python -c "import cv2; cap = cv2.VideoCapture(0); print(cap.isOpened())"
```

---

## Advanced Usage

### Custom Hyperparameter Tuning

**Modify `config/model_config.py`**:
```python
# Increase dropout for more regularization
DROPOUT_RATE = 0.6  # Default: 0.5

# Change batch size
BATCH_SIZE = 64  # Default: 32

# Adjust learning rates
LEARNING_RATE = 5e-5      # Warmup
FINE_TUNE_LR = 1e-5       # Fine-tuning

# Early stopping patience
EARLY_STOPPING_PATIENCE = 10  # Default: 5
```

### Custom Data Augmentation

**Modify `config/model_config.py`**:
```python
AUGMENTATION = {
    'rotation_range': 15,           # Increase rotation
    'width_shift_range': 0.15,      # More horizontal shift
    'height_shift_range': 0.15,     # More vertical shift
    'horizontal_flip': True,
    'brightness_range': (0.7, 1.3), # Wider brightness range
    'zoom_range': 0.15,             # More zoom variation
}
```

### Training with Custom Dataset

**If your dataset has different structure**:

1. **Organize images into emotion folders**:
```
my_dataset/
â”œâ”€â”€ angry/
â”œâ”€â”€ disgust/
â”œâ”€â”€ fear/
â”œâ”€â”€ happy/
â”œâ”€â”€ neutral/
â”œâ”€â”€ sad/
â””â”€â”€ surprised/
```

2. **Preprocess**:
```bash
python preprocess_dataset.py --input_dir my_dataset --output_dir preprocessed_my_dataset
```

3. **Create splits**:
```bash
python split_dataset.py --input_dir preprocessed_my_dataset --output_dir my_data_splits
```

4. **Update config** (`config/model_config.py`):
```python
DATA_SPLITS_DIR = Path('my_data_splits')
```

5. **Train normally**:
```bash
python train_phase3a.py --epochs 20 --models resnet50
```

### Resume Training from Checkpoint

**Resume Stage 2 training**:
```bash
python train_phase3b_progressive.py --epochs 30 --models resnet50 --resume
```

The `--resume` flag loads the existing progressive checkpoint and continues training.

### Export to Different Platforms

**Mobile (Android/iOS)**:
```bash
# Export to ONNX
python export_models.py --models resnet50 --format onnx

# Use in Android/iOS apps with ONNX Runtime Mobile
```

**Web (JavaScript)**:
```bash
# Export to ONNX
python export_models.py --models mobilenet --format onnx

# Use with ONNX.js in browser
```

**Embedded (Raspberry Pi, Jetson)**:
```bash
# Export to TorchScript for LibTorch on ARM
python export_models.py --models mobilenet --format torchscript
```

---

## Performance Metrics Summary

### Training Results (Final Models)

| Model | Parameters | Stage 1 | Stage 2 | Stage 3 | **Test Acc** | Train Time |
|-------|-----------|---------|---------|---------|-------------|-----------|
| ResNet50 | 23.8M | 26.9% | 61.3% | **74.4%** | **74.4%** | ~60 mins |
| ResNet18 | 11.2M | 30.9% | 64.6% | **68.9%** | **68.9%** | ~50 mins |
| MobileNet | 2.4M | 28.4% | 45.7% | **60.7%** | **60.7%** | ~40 mins |
| EfficientNet | 4.2M | 28.4% | 50.3% | **57.6%** | **57.6%** | ~45 mins |

### Inference Speed (NVIDIA RTX 4060)

| Model | PyTorch | TorchScript | ONNX Runtime |
|-------|---------|-------------|--------------|
| ResNet50 | 8.2 ms | 7.1 ms | 6.5 ms |
| ResNet18 | 5.4 ms | 4.8 ms | 4.2 ms |
| MobileNet | 3.1 ms | 2.7 ms | 2.3 ms |
| EfficientNet | 4.6 ms | 4.0 ms | 3.5 ms |

**Real-time FPS** (single face):
- MobileNet: ~320 FPS
- EfficientNet: ~220 FPS
- ResNet18: ~185 FPS
- ResNet50: ~120 FPS

---

## Next Steps

After completing this pipeline, consider:

1. **Collect More Data**: Expand dataset to 10,000+ images for better accuracy
2. **Implement Quantization**: INT8/FP16 for mobile deployment (2-4x faster)
3. **Model Pruning**: Reduce model size by 30-50%
4. **Ensemble Methods**: Combine multiple models for higher accuracy
5. **Real-World Testing**: Deploy in production and collect user feedback
6. **Continuous Learning**: Implement online learning for model improvement

---

## Support and Resources

**Documentation**:
- `README.md`: Project overview and API reference
- `PROJECT_TASKS.md`: Detailed task breakdown and status
- `TRAINING_GUIDE.md`: In-depth training documentation

**Checkpoints**:
- All trained models: `results/checkpoints/`
- Training logs: `results/logs/`
- Evaluation results: `results/evaluation/`

**Contact**:
- Project repository: [GitHub Link]
- Issues: [GitHub Issues]
- Email: [Your Email]

---

**Happy Coding! ðŸš€**
